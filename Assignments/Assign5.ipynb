{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"Deep learning (also known as deep structured learning) is part of a␣ ↪broader family\n",
    "of machine learning methods based on artificial neural␣ ↪networks with representation\n",
    "learning. Learning can be supervised,␣ ↪semi-supervised or unsupervised.\n",
    "Deep-learning architectures such as deep neural networks, deep belief networks,␣ ↪deep\n",
    "reinforcement learning, recurrent neural networks, convolutional neural␣ ↪networks and\n",
    "Transformers have been applied to fields including computer␣ ↪vision, speech recognition,\n",
    "natural language processing, machine␣ ↪translation, bioinformatics, drug design, medical\n",
    "image analysis, climate␣ ↪science, material inspection and board game programs, where\n",
    "they have␣ ↪produced results comparable to and in some cases surpassing human expert␣\n",
    "↪performance.\n",
    "\"\"\"\n",
    "dl_data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 81\n",
      "Vocabulary Sample: [('learning', 1), ('deep', 2), ('networks', 3), ('and', 4), ('as', 5), ('of', 6), ('neural␣', 7), ('↪networks', 8), ('supervised', 9), ('␣', 10)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(dl_data)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in dl_data]\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "            label_word.append(word)\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size,vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):',id2word[np.argwhere(y[0])[0][0]])\n",
    "    if i == 10:\n",
    "        break\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            8100      \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 81)                8181      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16281 (63.60 KB)\n",
      "Trainable params: 16281 (63.60 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size,input_length=window_size*2))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tLoss: 437.18522596359253\n",
      "\n",
      "Epoch: 2 \tLoss: 436.8022713661194\n",
      "\n",
      "Epoch: 3 \tLoss: 436.45318508148193\n",
      "\n",
      "Epoch: 4 \tLoss: 436.1224322319031\n",
      "\n",
      "Epoch: 5 \tLoss: 435.8004274368286\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids,window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "    if i % 100000 == 0:\n",
    "        print('Processed {} (context, word) pairs'.format(i))\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>deep</th>\n",
       "      <td>-0.001995</td>\n",
       "      <td>-0.011436</td>\n",
       "      <td>0.075044</td>\n",
       "      <td>0.047425</td>\n",
       "      <td>-0.011940</td>\n",
       "      <td>-0.015333</td>\n",
       "      <td>0.029801</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>-0.076849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053535</td>\n",
       "      <td>0.088919</td>\n",
       "      <td>0.053725</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>-0.020048</td>\n",
       "      <td>-0.003732</td>\n",
       "      <td>0.029792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>networks</th>\n",
       "      <td>0.034300</td>\n",
       "      <td>-0.052747</td>\n",
       "      <td>0.066491</td>\n",
       "      <td>0.078028</td>\n",
       "      <td>-0.066471</td>\n",
       "      <td>0.023391</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.061527</td>\n",
       "      <td>0.051237</td>\n",
       "      <td>-0.042125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065887</td>\n",
       "      <td>-0.012509</td>\n",
       "      <td>0.094437</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.021038</td>\n",
       "      <td>0.079572</td>\n",
       "      <td>0.082506</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>-0.023130</td>\n",
       "      <td>0.015624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.008408</td>\n",
       "      <td>-0.031874</td>\n",
       "      <td>0.044613</td>\n",
       "      <td>-0.077847</td>\n",
       "      <td>-0.017317</td>\n",
       "      <td>-0.072767</td>\n",
       "      <td>0.044673</td>\n",
       "      <td>-0.011651</td>\n",
       "      <td>-0.071850</td>\n",
       "      <td>-0.062053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082288</td>\n",
       "      <td>-0.046847</td>\n",
       "      <td>-0.096221</td>\n",
       "      <td>0.083486</td>\n",
       "      <td>0.082004</td>\n",
       "      <td>0.006684</td>\n",
       "      <td>-0.068995</td>\n",
       "      <td>0.093640</td>\n",
       "      <td>-0.032247</td>\n",
       "      <td>0.015511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>-0.000307</td>\n",
       "      <td>0.037113</td>\n",
       "      <td>-0.011164</td>\n",
       "      <td>-0.005272</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>-0.013050</td>\n",
       "      <td>-0.034871</td>\n",
       "      <td>-0.030539</td>\n",
       "      <td>-0.046336</td>\n",
       "      <td>-0.027210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>-0.006135</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>-0.027359</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>-0.035198</td>\n",
       "      <td>-0.013137</td>\n",
       "      <td>0.043853</td>\n",
       "      <td>0.039660</td>\n",
       "      <td>-0.012054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.043565</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>-0.022381</td>\n",
       "      <td>-0.033475</td>\n",
       "      <td>-0.049087</td>\n",
       "      <td>0.008909</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>-0.014609</td>\n",
       "      <td>0.024286</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>0.010598</td>\n",
       "      <td>0.026178</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>-0.012230</td>\n",
       "      <td>0.032175</td>\n",
       "      <td>-0.047500</td>\n",
       "      <td>-0.029658</td>\n",
       "      <td>-0.012575</td>\n",
       "      <td>-0.018918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \n",
       "deep     -0.001995 -0.011436  0.075044  0.047425 -0.011940 -0.015333  \\\n",
       "networks  0.034300 -0.052747  0.066491  0.078028 -0.066471  0.023391   \n",
       "and      -0.008408 -0.031874  0.044613 -0.077847 -0.017317 -0.072767   \n",
       "as       -0.000307  0.037113 -0.011164 -0.005272  0.018458 -0.013050   \n",
       "of        0.043565  0.009171  0.009804 -0.022381 -0.033475 -0.049087   \n",
       "\n",
       "                6         7         8         9   ...        90        91   \n",
       "deep      0.029801  0.006830  0.043598 -0.076849  ... -0.053535  0.088919  \\\n",
       "networks  0.005626  0.061527  0.051237 -0.042125  ... -0.065887 -0.012509   \n",
       "and       0.044673 -0.011651 -0.071850 -0.062053  ... -0.082288 -0.046847   \n",
       "as       -0.034871 -0.030539 -0.046336 -0.027210  ...  0.029320 -0.006135   \n",
       "of        0.008909  0.027836 -0.014609  0.024286  ... -0.016846  0.010598   \n",
       "\n",
       "                92        93        94        95        96        97   \n",
       "deep      0.053725  0.005008  0.006755  0.002106  0.079100 -0.020048  \\\n",
       "networks  0.094437  0.000709  0.021038  0.079572  0.082506 -0.001565   \n",
       "and      -0.096221  0.083486  0.082004  0.006684 -0.068995  0.093640   \n",
       "as        0.006043 -0.027359 -0.019847 -0.035198 -0.013137  0.043853   \n",
       "of        0.026178  0.005373 -0.012230  0.032175 -0.047500 -0.029658   \n",
       "\n",
       "                98        99  \n",
       "deep     -0.003732  0.029792  \n",
       "networks -0.023130  0.015624  \n",
       "and      -0.032247  0.015511  \n",
       "as        0.039660 -0.012054  \n",
       "of       -0.012575 -0.018918  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'deep': ['applied', 'bioinformatics', 'learning', 'inspection', 'medical']}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] for search_term in ['deep']}\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
